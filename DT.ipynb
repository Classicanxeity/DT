{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Q1. What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning model used for **classification and regression**. It works by repeatedly splitting the data into subsets based on the *best* feature that separates the target classes.\n",
        "\n",
        "### **How it Works (Classification)**\n",
        "\n",
        "1. The root node receives the entire dataset.\n",
        "2. The algorithm selects the feature that best separates the classes (using Gini or Entropy).\n",
        "3. Based on the selected feature, the dataset is split into branches.\n",
        "4. Each branch continues splitting until:\n",
        "\n",
        "   * All samples in a node belong to one class, or\n",
        "   * A stopping criterion (max depth, min samples) is reached.\n",
        "5. Leaf nodes assign a class label.\n",
        "\n",
        "### **Why It Works**\n",
        "\n",
        "A decision tree models human-like decision-making using simple yes/no questions. It learns rules such as:\n",
        "\n",
        "> If petal length < 2.5 → Iris-setosa\n",
        "> Else if petal width < 1.8 → Iris-versicolor\n",
        "> Else → Iris-virginica\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Easy to understand and visualize\n",
        "* Handles numerical and categorical data\n",
        "* Requires little data preprocessing\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "* Prone to overfitting\n",
        "* High variance\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S5Dofz8xLcI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q2. Explain Gini Impurity and Entropy. How do they impact splits in a Decision Tree?**\n",
        "\n",
        "### **1) Gini Impurity**\n",
        "\n",
        "Measures how often a randomly chosen sample would be incorrectly classified.\n",
        "\n",
        "[\n",
        "Gini = 1 - \\sum p_i^2\n",
        "]\n",
        "\n",
        "* Range: 0 (pure) to 0.5 (impure in binary class)\n",
        "* Faster to compute than entropy\n",
        "\n",
        "### **2) Entropy**\n",
        "\n",
        "Measures the impurity using information theory.\n",
        "\n",
        "[\n",
        "Entropy = -\\sum p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "* Range: 0 (pure) to 1 (high impurity)\n",
        "\n",
        "### **Impact on Splits**\n",
        "\n",
        "Decision Trees choose **splits that reduce impurity the most**.\n",
        "\n",
        "* Using **Gini**, the tree prefers splits that isolate the most frequent class.\n",
        "* Using **Entropy**, the tree prefers splits that maximize “information gain.”\n",
        "\n",
        "In practice:\n",
        "\n",
        "* Gini = default in scikit-learn\n",
        "* Both give similar splits\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TVSo_iZ2NOTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Q3. Difference between Pre-Pruning and Post-Pruning. Give one advantage each.**\n",
        "\n",
        "### **Pre-Pruning (Early Stopping)**\n",
        "\n",
        "Stops the tree **before** it grows too deep.\n",
        "\n",
        "Methods:\n",
        "\n",
        "* max_depth\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "\n",
        "**Advantage:**\n",
        "✔ Prevents overfitting early and reduces training time.\n",
        "\n",
        "---\n",
        "\n",
        "### **Post-Pruning (Cost Complexity Pruning)**\n",
        "\n",
        "Grow a full tree first → then prune useless branches.\n",
        "\n",
        "Methods:\n",
        "\n",
        "* Reduced error pruning\n",
        "* Cost complexity pruning (ccp_alpha)\n",
        "\n",
        "**Advantage:**\n",
        "✔ Produces simpler, more generalizable models.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Bt89yDS_NUp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q4. What is Information Gain and why is it important?**\n",
        "\n",
        "**Information Gain (IG)** measures how much a feature reduces impurity.\n",
        "\n",
        "[\n",
        "IG = Entropy(parent) - \\sum \\frac{N_i}{N} Entropy(child_i)\n",
        "]\n",
        "\n",
        "### **Importance**\n",
        "\n",
        "* Higher IG = better feature for splitting\n",
        "* Helps select the **most informative feature**\n",
        "* Ensures the model learns meaningful patterns\n",
        "\n",
        "Decision Trees repeatedly choose the split with **maximum IG**, ensuring fast and accurate classification.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NlXa010uNiFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q5. Real-World Applications of Decision Trees + Advantages & Limitations**\n",
        "\n",
        "### **Applications**\n",
        "\n",
        "* Medical diagnosis\n",
        "* Customer churn prediction\n",
        "* Credit risk scoring\n",
        "* Fraud detection\n",
        "* Loan approval systems\n",
        "* Product recommendation\n",
        "* Agriculture (crop disease prediction)\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Interpretable (explainable AI)\n",
        "* Handles both numerical & categorical data\n",
        "* No need for feature scaling\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "* Overfitting\n",
        "* High variance\n",
        "* Unstable with small dataset changes\n",
        "* Biased towards features with many levels\n",
        "\n",
        "---\n",
        "\n",
        "# **Programming Questions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6TVrcDJ1NmuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q6. Python Program – Train Decision Tree (Gini) on Iris Dataset**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XLGOZNffPwey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmZwb41BP2Ur",
        "outputId": "d346b018-0b58-48c9-f243-bf1a59398b45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9bn1VkqPwrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q7.Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "K-DMyorIQDDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# full tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "\n",
        "# depth 3 tree\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_pred = pruned_tree.predict(X_test)\n",
        "\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, full_pred))\n",
        "print(\"Depth=3 Accuracy:\", accuracy_score(y_test, pruned_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ-_R9UwQPIp",
        "outputId": "ccaf1aa1-50e7-4c10-a541-98f9065070ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q.8.Write a Python program to:**\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "FEjhugzyQT9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95iwLCzzQUEG",
        "outputId": "f2ecc709-247a-4370-b816-37ac019a4af6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9.Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "PnKlqkVGQ-mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load Iris data specifically for this question\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "# Train-test split for Iris data\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                    param_grid=params,\n",
        "                    cv=5,\n",
        "                    scoring='accuracy')\n",
        "\n",
        "grid.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsjOPlGCQg0E",
        "outputId": "7c71ccb9-ae36-4111-b18e-7694090cad1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Accuracy: 0.9416666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Q10. Healthcare Disease Prediction – End-to-End Workflow**\n",
        "\n",
        "### **Step 1: Handle Missing Values**\n",
        "\n",
        "* Numerical → impute using mean/median\n",
        "* Categorical → impute using most frequent value\n",
        "* Optional: advanced imputation (KNN Imputer)\n",
        "\n",
        "### **Step 2: Encode Categorical Features**\n",
        "\n",
        "* Label Encoding for binary categories\n",
        "* One-Hot Encoding for multi-category features\n",
        "\n",
        "### **Step 3: Train Decision Tree Model**\n",
        "\n",
        "* Define X and y\n",
        "* Split dataset\n",
        "* Fit DecisionTreeClassifier\n",
        "* Evaluate using accuracy, F1-score, confusion matrix\n",
        "\n",
        "### **Step 4: Hyperparameter Tuning**\n",
        "\n",
        "Parameters to tune:\n",
        "\n",
        "* max_depth\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* criterion (gini/entropy)\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "### **Step 5: Model Evaluation**\n",
        "\n",
        "* Classification accuracy\n",
        "* ROC-AUC\n",
        "* Precision-Recall\n",
        "* Confusion matrix\n",
        "* Feature importance visualisation\n",
        "\n",
        "### **Business Value**\n",
        "\n",
        "* Helps doctors flag high-risk patients early\n",
        "* Improves diagnosis speed\n",
        "* Reduces manual workload\n",
        "* Supports evidence-based treatment\n",
        "* Saves cost by early detection\n",
        "* Enhances patient care quality\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CeILBvcmRG_y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kURIaI3ERHGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}